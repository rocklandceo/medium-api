<h1>Custom Memory for ChatGPT API</h1>
<h3>A Gentle Introduction to LangChain Memory Types</h3>
<p><img alt="Self-made gif." src="https://miro.medium.com/1*B275_3l9j_mw4-W9G7ZCqA.gif" /></p>
<p>If you have ever used the OpenAI API, I am sure you have noticed the catch.</p>
<p><em>Got it?</em></p>
<p><em>Right!</em> Every time you call the ChatGPT API, the model has no memory of the previous requests you have made. In other words: <strong>each API call is a standalone interaction</strong>.</p>
<p>And that is definitely annoying when you need to perform follow-up interactions with the model. A chatbot is the golden example where follow-up interactions are needed.</p>
<p>In this article, we will explore how to give memory to ChatGPT when using the OpenAI API, so that it remembers our previous interactions.</p>
<h2>Warm-Up!</h2>
<p>Let's perform some interactions with the model so that we experience this default no-memory phenomenon:</p>
<p>```makefile
prompt = "My name is Andrea"
response = chatgpt_call(prompt)
print(response)</p>
<h1>Output: Nice to meet you, Andrea! How can I assist you today?</h1>
<p>```</p>
<p>But when asked a follow-up question:</p>
<p>```makefile
prompt = "Do you remember my name?"
response = chatgpt_call(prompt)
print(response)</p>
<h1>Output: I'm sorry, as an AI language model, I don't have the ability</h1>
<h1>to remember specific information about individual users.</h1>
<p>```</p>
<p><em>Right,</em> so in fact the model does not remember my name even though it was given on the first interaction.</p>
<p><strong>Note:</strong> The method <code>chatgpt_call()</code> is just a wrapper around the OpenAI API. We already gave a shot on how easily call GPT models at <a href="https://medium.com/forcodesake/chatgpt-api-calls-introduction-chatgpt3-chatgpt4-ai-d19b79c49cc5">ChatGPT API Calls: A Gentle Introduction</a> in case you want to check it out!</p>
<p>Some people normally work around this memoryless situation by pre-feeding the previous conversation history to the model every time they do a new API call. Nevertheless, this practice is not cost-optimized and it has certainly a limit for long conversations.</p>
<p>In order to create a memory for ChatGPT so that it is aware of the previous interactions, we will be using the popular <code>langchain</code> framework. This framework allows you to easily manage the ChatGPT conversation history and optimize it by choosing the right memory type for your application.</p>
<h2><strong>LangChain Framework</strong></h2>
<p>The <code>langchain</code> framework's <strong>purpose is to assist developers when building applications powered by Large Language Models (LLMs).</strong></p>
<p><img alt="Self-made screenshot from the official LangChain GitHub repository." src="https://miro.medium.com/1*6NnR65TjZ5F3mfwiGLXiGw.png" /></p>
<p>According to their <a href="https://github.com/hwchase17/langchain">GitHub description</a>:</p>
<blockquote>
<p>Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.</p>
<p>This library aims to assist in the development of those types of applications.</p>
</blockquote>
<p>They claim that building an application by only using LLMs may be insufficient. We found that too when doing follow-up interactions with the model by using the OpenAI API only.</p>
<h3>Framework Setup</h3>
<p>Getting the <code>langchain</code> library up and running in Python is simple. As for any other Python library, we can install it with <code>pip</code>:</p>
<p><code>typescript
pip install langchain</code></p>
<p>LangChain calls the OpenAI API behind the scenes. Therefore, it is necessary to set your OpenAI API key as an environment variable called <code>OPENAI_API_KEY</code>. Check out <a href="https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197">A Step-by-Step Guide to Getting Your API Key</a> if you need some guidance for getting your OpenAI key.</p>
<h3>LangChain: Basic Calls</h3>
<p>Let's start by setting up a basic API call to ChatGPT using LangChain.</p>
<p>This task is pretty straightforward since the module <code>langchain.llms</code> already provides an <code>OpenAI()</code> method for this purpose:</p>
<p>```python</p>
<h1>Loads OpenAI key from the environment</h1>
<p>from langchain.llms import OpenAI
chatgpt = OpenAI()
```</p>
<p>Once the desired model is loaded, we need to start the so-called <em>conversation chain</em>. LangChain also provides a module for that purpose:</p>
<p><code>python
from langchain.chains import ConversationChain
conversation = ConversationChain(llm=chatgpt)</code></p>
<p>Let's define the conversation with <code>verbose=True</code> to observe the reasoning process of the model.</p>
<p>Finally, <code>langchain</code> provides a <code>.predict()</code> method to send your desired prompt to ChatGPT and get its completion back. <em>Let's try it!</em></p>
<p><code>python
conversation.predict(input="Hello, we are ForCode'Sake! A Medium publication with the objective of democratizing the knowledge of data!")</code></p>
<p><em>Let's do a follow-up interaction!</em></p>
<p>```python
conversation.predict(input="Do you remember our name?")</p>
<h1>Output: " Hi there! It's great to meet you.</h1>
<h1>I'm an AI that specializes in data analysis.</h1>
<h1>I'm excited to hear more about your mission in democratizing data knowledge.</h1>
<h1>What inspired you to do this?"</h1>
<p>```</p>
<p>We can see that <strong>the model is capable of handling follow-up interactions without problems when using <code>langchain</code></strong>.</p>
<h2>LangChain Memory Types</h2>
<p>As we have observed, LangChain conversation chains already keep track of the <code>.predict</code> calls for a declared <code>conversation</code>. However, <strong>the default conversation chain stores each and every interaction we have had with the model</strong>.</p>
<p>As we have briefly discussed at the beginning of the article, storing all the interactions with the model can <strong>quickly escalate to a considerable amount of tokens to process every time we prompt the model</strong>. It is essential to bear in mind that ChatGPT has a token limit per interaction.</p>
<p>In addition, the <strong>ChatGPT usage cost also depends on the number of tokens</strong>. Processing all the conversation history in each new interaction is likely to be expensive over time.</p>
<p>To overcome these limitations, <code>langchain</code> implements different types of memories to use in your application.</p>
<p><em>Let's explore them!</em></p>
<h3>#1. Complete Interactions</h3>
<p>Although the default behavior of LangChain is to store all the past interactions, this memory type can be explicitly declared. It is the so-called <code>ConversationBufferMemory</code>, and it simply fills a buffer with all our previous interactions:</p>
<p><code>python
from langchain.memory import ConversationBufferMemory
memory=ConversationBufferMemory()</code></p>
<p>Declaring the memory type allows us to have some additional control over the ChatGPT memory. For example, we can check the buffer content at any time with <code>memory.buffer</code> or <code>memory.load_memory_variables({})</code>.</p>
<p>In addition, we can add extra information to the buffer without doing a real interaction with the model:</p>
<p><code>css
memory.save_context({"input": "Hi"},
                    {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"},
                    {"output": "Cool"})</code></p>
<p>If you do not need to manipulate the buffer in your application, you might be good to go with the default memory and no explicit declaration. <strong>Although I really recommend it for debugging purposes!</strong></p>
<h3>#2. Interactions within a window</h3>
<p>One less costly alternative is storing only a certain amount of previous interactions (<code>k</code>) with the model. That is the so-called <em>window</em> of interaction.</p>
<p>When conversations grow big enough, it might be sufficient for your application that the model only remembers the most recent interactions. For those cases, the <code>ConversationBufferWindowMemory</code> module is available.</p>
<p><em>Let's explore its behavior!</em></p>
<p>Firstly, we need to load the <code>llm</code> model and the new type of <code>memory</code>. In this case, we are setting <code>k=1</code> which means that only the previous iteration will be kept in memory:</p>
<p>```python</p>
<h1>OpenAI key from environment</h1>
<p>from langchain.llms import OpenAI
llm = OpenAI()
```</p>
<p><code>java
from langchain.memory import ConversationBufferWindowMemory
memory = ConversationBufferWindowMemory(k=1)</code></p>
<p>Secondly, let's add some context to our conversation as shown in the previous section:</p>
<p><code>python
memory.save_context({"input": "Hi"},
                    {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"},
                    {"output": "Cool"})</code></p>
<p>Although we have stored two interactions in our conversation history, due to the fact that we have set <code>k=1</code>, the model will only remember the last interaction <code>{"input": "Not much, just hanging"},
 {"output": "Cool"}</code>.</p>
<p>To prove it, let's check what the model has in memory:</p>
<p>```python
memory.load_memory_variables({})</p>
<h1>Output: {'history': 'Human: Not much, just hanging\nAI: Cool'}</h1>
<p>```</p>
<p>We can further prove it by asking a follow-up question setting <code>verbose=True</code>, so that we can observe the stored interactions:</p>
<p><code>python
conversation.predict(input="Can you tell me a joke?")</code></p>
<p>And the verbose output is the following:</p>
<script src="https://gist.github.com/aandvalenzuela/0b75634f6407495de1370cf5cfe337d8.js"></script>
<p>As we can observe, <strong>the model only remembers the previous interaction</strong>.</p>
<h3>#3. Summary of the interactions</h3>
<p>I am sure you are now thinking that <strong>completely deleting old interactions with the model might be a bit risky for some applications</strong>.</p>
<p>Let's imagine a customer service chatbot that asks to the user its contract number in the first place. The model must not forget this information, no matter which interaction number it has.</p>
<p>For that purpose, there is a memory type that uses the model itself to generate a summary of the previous interactions. Therefore, the model only stores a summary of the conversation in memory.</p>
<p>This optimized memory type is the so-called <code>ConversationSummaryBufferMemory</code>. It also allows to store the complete most recent interactions up to a maximum number of tokens (given by <code>max_token_limit</code>) together with the summary of the previous ones.</p>
<p><em>Let's observe this memory behavior in practice!</em></p>
<p><code>javascript
from langchain.memory import ConversationSummaryBufferMemory</code></p>
<p>Let's create a conversation with quite some content so that we can explore the summary capabilities:</p>
<p>```python
schedule = "There is a meeting at 8am with your product team. \
You will need your powerpoint presentation prepared. \
9am-12pm have time to work on your LangChain \
project which will go quickly because Langchain is such a powerful tool. \
At Noon, lunch at the Italian restaurant with a customer who is driving \
from over an hour away to meet you to understand the latest in AI. \
Be sure to bring your laptop to show the latest LLM demo."</p>
<p>memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
memory.save_context({"input": "Hello"}, {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"},
                    {"output": "Cool"})
memory.save_context({"input": "What is on the schedule today?"}, 
                    {"output": f"{schedule}"})
```</p>
<p>Now, when checking the memory content with <code>memory.load_memory_variables({})</code> , we will see the actual summary of our interactions:</p>
<p><code>json
{
  'history': 'System: 
  \nThe human greets the AI and asks what is on the schedule for the day. 
  The AI responds with "Cool".\n
  AI: There is a meeting at 8am with your product team. 
  You will need your powerpoint presentation prepared. 
  9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. 
  At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. 
  Be sure to bring your laptop to show the latest LLM demo.'
}</code></p>
<p><em>The summary sounds nice, isn't it?</em></p>
<p>Let's perform a new interaction!</p>
<p>```python
llm = OpenAI()</p>
<p>conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)
```</p>
<p>And the verbose output looks as follows:</p>
<script src="https://gist.github.com/aandvalenzuela/162243fbca72b86439be4e9597dfd1b0.js"></script>
<p>As we can observe from the example, <strong>this memory type allows the model to keep important information, while reducing the irrelevant information</strong> and, therefore, the amount of used tokens in each new interaction.</p>
<h2>Summary</h2>
<p>In this article, we have seen <strong>different ways to create a memory for our GPT-powered application</strong> depending on our needs.</p>
<p>By <strong>using the LangChain framework instead of bare API calls to the OpenAI API</strong>, we get rid of simple problems such as making the model aware of the previous interactions.</p>
<p>Despite the fact that the default memory type of LangChain might be already enough for your application, I really encourage you to estimate the average length of your conversations. It is a nice exercise to compare the average number of tokes used - and therefore the cost! - with the usage of the summary memory. <strong>You can get full model performance at a minimal cost!</strong></p>
<p>It seems to me that the LangChain framework has a lot to give us regarding GPT models. <em>Have you already discovered another handy functionality?</em></p>
<hr />
<p>That is all! Many thanks for reading!</p>
<p>I hope this article helps you when <strong>building ChatGPT applications!</strong></p>
<p>You can also subscribe to my <strong><a href="https://medium.com/@andvalenzuela/subscribe">Newsletter</a></strong> to stay tuned for new content. <strong>Especially</strong>, <strong>if you are interested in articles about ChatGPT</strong>:</p>
<blockquote>
<p><a href="https://towardsdatascience.com/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8"><strong>ChatGPT Moderation API: Input/Output Control</strong></a></p>
<p><a href="https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54"><strong>Unleashing the ChatGPT Tokenizer</strong></a></p>
<p><a href="https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce"><strong>Mastering ChatGPT: Effective Summarization with LLMs</strong></a></p>
</blockquote>
<p>Also towards a <strong>responsible AI</strong>:</p>
<blockquote>
<p><a href="https://towardsdatascience.com/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6"><strong>What ChatGPT Knows about You: OpenAI's Journey Towards Data Privacy</strong></a></p>
</blockquote>