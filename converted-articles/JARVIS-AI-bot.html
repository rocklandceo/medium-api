<h3>Artificial Intelligence | GPT-3</h3>
<h1>Bring Tony Stark's JARVIS to Life: Build a Personal AI Assistant with Python, React, and GPT-3</h1>
<h3>From Idea to Reality: How I Built My Own Personal AI Assistant Web App</h3>
<p><img alt="https://labs.openai.com/s/OpIpRPXhHAB77gn6EVF0FOeJ" src="https://miro.medium.com/1*YECgOJd1rsiH0v3_Bhb4vg.png" /></p>
<p>Do you ever wish you could have a personal AI assistant like Tony Stark's J.A.R.V.I.S. or Iron Man' F.R.I.D.A.Y to help you with your tasks, answer your questions, and keep you company?</p>
<p>Well, you are in luck because I have turned this sci-fi dream into a reality! That is at least some parts of it.</p>
<p>In this article, I will show you how to build your very own AI assistant using Python FastAPI, ReactJS, and the powerful GPT-3 language model.</p>
<p>You will see a live demo of the AI assistant in action and explore the system design and architecture. I will break down how the backend and frontend work, so you will have a good understanding of what's going on under the hood.</p>
<p>I will also provide you with the open-source Github code to get you started.</p>
<p>So, whether you are a seasoned developer or a curious beginner, you can build your very own AI assistant with ease.</p>
<hr />
<h2><strong>From Idea to Reality: Turning a Dreamy AI Assistant into a Tangible Web App!</strong></h2>
<p>In my previous article, <em><a href="https://medium.com/gitconnected/creating-your-own-ai-powered-second-brain-a-guide-with-python-and-chatgpt-f5547ef7e136">Creating Your Own AI-Powered Second Brain</a></em>, I explored how to create an AI-powered second brain using Python and ChatGPT. This second brain was able to remember and organize information based on context data provided by the user.</p>
<p>It was a successful proof-of-concept. Looking at the reading stats and engagement, it seems like you folks found it interesting too.</p>
<p>In this post, we take things to the next level by actually building a personal AI assistant that you can talk to, listen to, and ask questions of - all in natural language.</p>
<p>Plus, with the power of GPT-3 and web scraping, this AI assistant can deliver even more insights and answers beyond the user-provided context data.</p>
<p>So, are you ready to revolutionize the way you work and live? Let's get started.</p>
<hr />
<h2><strong>The Real Deal: Witness the AI Assistant in Action</strong></h2>
<p>It's a little tricky to showcase the power of the assistant here on Medium.</p>
<p>Let me explain what are the steps involved first, and then I will leave you with a GIF and a YouTube video to see the full thing in action. Here are the steps:</p>
<ol>
<li>
<p>Hover your mouse on the "Say Something" button</p>
</li>
<li>
<p>Recording starts</p>
</li>
<li>
<p>Speak your question towards the microphone</p>
</li>
<li>
<p>Move your mouse cursor away from the button</p>
</li>
<li>
<p>Backend magic happens!</p>
</li>
<li>
<p>Your AI assistant speaks the answer to you through the speakers</p>
</li>
<li>
<p>You also get a text transcript of the question/answer in the UI</p>
</li>
</ol>
<p>To get the best experience, I would recommend checking the 50 seconds <a href="https://youtu.be/ncTRdQPs0Ug">YouTube video below</a>.</p>
<iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FncTRdQPs0Ug&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DncTRdQPs0Ug&image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FncTRdQPs0Ug%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube" title="" height="480" width="854"></iframe>

<p>Given the audio/video nature of the feature, a GIF won't really do it much justice, but here's one if you don't prefer YouTube:</p>
<p><img alt="Author's generated GIF to showcase the personal assistant web app's functionality" src="https://miro.medium.com/1*2eKVrYZ7rCo6R0Gj6otDyg.gif" /></p>
<hr />
<h2><strong>Behind the Scenes: A Look at the System Design and Architecture of the AI Assistant</strong></h2>
<p>Now, let's move our attention to the technical details.</p>
<p>If I had to break down the system into multiple components, this is how it would look like:</p>
<ul>
<li>
<p>GPT-3 as the Large Language Model (LLM)</p>
</li>
<li>
<p>Llama-Index to vectorize context data and pass it to GPT-3</p>
</li>
<li>
<p>Python FastAPI server to interact with the trained LLM model</p>
</li>
<li>
<p>ReactJS &amp; ChakraUI to build a frontend UI</p>
</li>
<li>
<p>Webkit SpeechRecognition library for voice input</p>
</li>
<li>
<p>Webkit SpeechSynthesisUtterance library for text to speech</p>
</li>
</ul>
<p>If you put all these together, this is how the system looks.</p>
<p><img alt="Author's created system design diagram on Miro" src="https://miro.medium.com/1*4F_y6YDpHNtpkh5UddHxJQ.png" /></p>
<p>Read the system design diagram from left to right, top to bottom.</p>
<p>Now that you have a bigger picture idea about how the system works, let's zoom into both the frontend and backend separately to get a deeper understanding.</p>
<hr />
<h2><strong>The Backend: Learn How the Python FastAPI and GPT-3 Powers the AI Assistant</strong></h2>
<p>In the last few months, ChatGPT has taken over the world, quite literally.</p>
<p>You can ask it to do your homework, prepare your presentations, write your SQL queries, help you write code, generate realistic images and videos - the list goes on and on.</p>
<p>Even though it can do all these different things, it still struggles when you ask it questions about your life - what did you eat yesterday? Who did you meet last week? Did you buy your medicines?</p>
<p>ChatGPT cannot answer these because it does not have any visibility into your personal life.</p>
<p>You need to give it your personal data for it to be able to help you. That's where the following comes in:</p>
<ul>
<li>
<p>A text file with your journal entries</p>
</li>
<li>
<p>Llama Index to read this text file, vectorize the data, and pass it as context to GPT-3</p>
</li>
</ul>
<p>A combination of these two gives GPT-3 what it needs to answer any question you have about your personal life.</p>
<p>Of course, it's not magic. You need the data to be existing in the journal to begin with, for GPT-3 to be able to help you.</p>
<p>The first step is to train the GPT-3 model on this data. Next, you save the trained model on your server.</p>
<p>```python
from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader</p>
<h1>Load data from the journal text file</h1>
<p>documents = SimpleDirectoryReader("./data").load_data()</p>
<h1>Create a simple vector index</h1>
<p>index = GPTSimpleVectorIndex(documents)
index.save_to_disk("generated_index.json")</p>
<h1>Create an infinite loop asking for user input and then breaking out of the loop when the response is empty</h1>
<p>while True:
    query = input("Ask a question: ")
    if not query:
        print("Goodbye")
        break
    # query the index with the question and print the result
    result = index.query(query)
    print(result)
```</p>
<p>Now, you build a very simple FastAPI endpoint to interact with this saved model. The endpoint logic is straightforward:</p>
<ol>
<li>
<p>Pass the user question coming from the web app</p>
</li>
<li>
<p>Ask the question to the saved GPT-3 model</p>
</li>
<li>
<p>Return the answer to the client in JSON</p>
</li>
</ol>
<p>```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from llama_index import GPTSimpleVectorIndex</p>
<p>app = FastAPI()</p>
<h1>Define allowed origins</h1>
<p>origins = [
    "http://localhost:3000",
    "http://localhost:5000",
    "http://localhost:8000",
    "http://localhost:8080",
]</p>
<h1>Add CORS middleware</h1>
<p>app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["<em>"],
    allow_headers=["</em>"],
)</p>
<p>@app.get("/answers")
async def get_answer(question: str):
    index = GPTSimpleVectorIndex.load_from_disk("generated_index.json")
    answer = index.query(question)</p>
<pre><code>return {"answer": answer.response}
</code></pre>
<p>```</p>
<p>After this, the ball is in the client's court to present the data back to the user. Let's see how that's done.</p>
<hr />
<h2><strong>The Frontend: Discover How ReactJS Brings the AI Assistant to Life with a Stunning UI</strong></h2>
<p>The user interacts with the web app.</p>
<p>The web app has fundamentally 4 jobs:</p>
<ol>
<li>
<p>Take the user's question through the microphone and turn it into text</p>
</li>
<li>
<p>Pass the question to the server through an API call</p>
</li>
<li>
<p>Transform the answer coming from the server from text to speech, and produce output through the user's speakers</p>
</li>
<li>
<p>Show the transcript to the user when doing speech-to-text and text-to-speech</p>
</li>
</ol>
<p>```javascript
import React, { useState, useEffect } from "react";
import { Button, VStack, Center, Heading, Box, Text } from "@chakra-ui/react";</p>
<p>function App() {
  const [transcript, setTranscript] = useState("");
  const [answer, setAnswer] = useState("");
  const [isRecording, setIsRecording] = useState(false);
  const [buttonText, setButtonText] = useState("Say Something");
  const [recognitionInstance, setRecognitionInstance] = useState(null);</p>
<p>useEffect(() =&gt; {
    const recognition = new window.webkitSpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = "en-US";</p>
<pre><code>recognition.onresult = (event) =&gt; {
  let interimTranscript = "";
  let finalTranscript = "";

  for (let i = event.resultIndex; i &lt; event.results.length; i++) {
    const transcript = event.results[i][0].transcript;
    if (event.results[i].isFinal) {
      finalTranscript += transcript + " ";
    } else {
      interimTranscript += transcript;
    }
  }

  setTranscript(finalTranscript);
};

setRecognitionInstance(recognition);
</code></pre>
<p>}, []);</p>
<p>const recordAudio = () =&gt; {
    setAnswer("");
    setButtonText("Recording...");
    setIsRecording(!isRecording);
    recognitionInstance.start();
  };</p>
<p>const stopAudio = async () =&gt; {
    setButtonText("Say Something");
    setIsRecording(!isRecording);
    recognitionInstance.stop();</p>
<pre><code>const response = await fetch(
  `http://127.0.0.1:8000/answers?question=${transcript}`
);
const data = await response.json();
setAnswer(data["answer"]);

const utterance = new SpeechSynthesisUtterance(data["answer"]);

window.speechSynthesis.speak(utterance);
</code></pre>
<p>};</p>
<p>return (
    <Box
      bg="black"
      h="100vh"
      display="flex"
      justifyContent="center"
      alignItems="center"
      padding="20px"
    >
      <Center>
        <VStack spacing={12}>
          <Heading color="red.500" fontSize="8xl">
            üëã  I am your personal assistant ü§ñ
          </Heading>
          <Button
            colorScheme="red"
            width="300px"
            height="150px"
            onMouseOver={recordAudio}
            onMouseLeave={stopAudio}
            fontSize="3xl"
          >
            {buttonText}
          </Button>
          {transcript &amp;&amp; (
            <Text color="whiteAlpha.500" fontSize="2xl">
              Question: {transcript}
            </Text>
          )}
          {answer &amp;&amp; (
            <Text color="white" fontSize="3xl">
              <b>Answer:</b> {answer}
            </Text>
          )}
        </VStack>
      </Center>
    </Box>
  );
}</p>
<p>export default App;
```</p>
<p>There are some really fancy libraries you can use to generate amazing AI voices.</p>
<p>I kept things very simple and used Webkit libraries that are baked into the browser.</p>
<hr />
<h2><strong>Give it a Go: Get Started with Your Own Personal AI Assistant Today!</strong></h2>
<p>If you have reached this far into the article, thank you so much for reading.</p>
<p>I hope you found this valuable and insightful.</p>
<p>I open-sourced the on my personal <strong><a href="https://github.com/irtiza07/personal-assistant-ai-www">GitHub repo</a></strong>. If you know your way around code, I would highly suggest cloning it and getting started with your own personal AI assistant!</p>
<p>I will close it with a pro tip: To make the assistant most helpful, export data from your task manager and calendar, and put it in your text file. I trained my model on data from ClickUp and Google Calendar. It was insanely useful!</p>
<p>Super excited to hear from you :) Thank you for reading.</p>
<p>If you enjoyed it, please consider clapping and following me on Medium.</p>
<hr />
<h2>Level Up Coding</h2>
<p>Thanks for being a part of our community! Before you go:</p>
<ul>
<li>
<p>üëè  Clap for the story and follow the author üëâ </p>
</li>
<li>
<p>üì∞  View more content in the <a href="https://levelup.gitconnected.com/?utm_source=pub&amp;utm_medium=post">Level Up Coding publication</a></p>
</li>
<li>
<p>üí∞  Free coding interview course ‚áí <a href="https://skilled.dev/?utm_source=luc&amp;utm_medium=article">View Course</a></p>
</li>
<li>
<p>üîî  Follow us: <a href="https://twitter.com/gitconnected">Twitter</a> | <a href="https://www.linkedin.com/company/gitconnected">LinkedIn</a> | <a href="https://newsletter.levelup.dev">Newsletter</a></p>
</li>
</ul>
<p>üöÄüëâ  J<strong><a href="https://jobs.levelup.dev/talent/welcome?referral=true">oin the Level Up talent collective and find an amazing job</a></strong></p>